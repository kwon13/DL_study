{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten, Embedding "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 단어 토큰화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러오기\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 전처리 할 텍스트 지정\n",
    "text='꿈이 있으면 휴식은 사치일 뿐'\n",
    "\n",
    "# 해당 텍스트 토큰화\n",
    "result = text_to_word_sequence(text)\n",
    "print(text)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "꿈이 있으면 휴식은 사치일 뿐\n",
      "['꿈이', '있으면', '휴식은', '사치일', '뿐']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# 단어 빈도수 세기\n",
    "\n",
    "# 세 개의 문장 정의\n",
    "docs=['공부는 공부를 하기 위해서가 아닌 문제를 해결하기 위해서 해야 합니다',\n",
    "    '해결하려는 문제가 나와 관련된 문제일수록 더욱 노력하게 됩니다',\n",
    "    '더 많은 공부보단 다양한 경험을 하면서 자신이 할 수 있는 한계점을 찾고 해결하기 위해 노력합시다']\n",
    "\n",
    "# 토큰화 함수를 이용해 전처리 하는 과정\n",
    "token=Tokenizer()           # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs)    # 토큰화 함수에 문장 적용\n",
    "\n",
    "# 각 단어 빈도수\n",
    "print(token.word_counts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('공부는', 1), ('공부를', 1), ('하기', 1), ('위해서가', 1), ('아닌', 1), ('문제를', 1), ('해결하기', 2), ('위해서', 1), ('해야', 1), ('합니다', 1), ('해결하려는', 1), ('문제가', 1), ('나와', 1), ('관련된', 1), ('문제일수록', 1), ('더욱', 1), ('노력하게', 1), ('됩니다', 1), ('더', 1), ('많은', 1), ('공부보단', 1), ('다양한', 1), ('경험을', 1), ('하면서', 1), ('자신이', 1), ('할', 1), ('수', 1), ('있는', 1), ('한계점을', 1), ('찾고', 1), ('위해', 1), ('노력합시다', 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 각 단어 빈도수\n",
    "print(token.word_counts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('공부는', 1), ('공부를', 1), ('하기', 1), ('위해서가', 1), ('아닌', 1), ('문제를', 1), ('해결하기', 2), ('위해서', 1), ('해야', 1), ('합니다', 1), ('해결하려는', 1), ('문제가', 1), ('나와', 1), ('관련된', 1), ('문제일수록', 1), ('더욱', 1), ('노력하게', 1), ('됩니다', 1), ('더', 1), ('많은', 1), ('공부보단', 1), ('다양한', 1), ('경험을', 1), ('하면서', 1), ('자신이', 1), ('할', 1), ('수', 1), ('있는', 1), ('한계점을', 1), ('찾고', 1), ('위해', 1), ('노력합시다', 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 총 문장 수\n",
    "token.document_count"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 각 단어가 몇개의 문장에 포함되어 있는가\n",
    "print(token.word_docs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<class 'int'>, {'아닌': 1, '문제를': 1, '위해서': 1, '해야': 1, '공부를': 1, '위해서가': 1, '공부는': 1, '하기': 1, '해결하기': 2, '합니다': 1, '관련된': 1, '해결하려는': 1, '나와': 1, '문제일수록': 1, '더욱': 1, '문제가': 1, '노력하게': 1, '됩니다': 1, '다양한': 1, '자신이': 1, '경험을': 1, '수': 1, '찾고': 1, '하면서': 1, '있는': 1, '더': 1, '노력합시다': 1, '한계점을': 1, '할': 1, '많은': 1, '공부보단': 1, '위해': 1})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 각 단어의 매겨진 인덱스 값\n",
    "print(token.word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'해결하기': 1, '공부는': 2, '공부를': 3, '하기': 4, '위해서가': 5, '아닌': 6, '문제를': 7, '위해서': 8, '해야': 9, '합니다': 10, '해결하려는': 11, '문제가': 12, '나와': 13, '관련된': 14, '문제일수록': 15, '더욱': 16, '노력하게': 17, '됩니다': 18, '더': 19, '많은': 20, '공부보단': 21, '다양한': 22, '경험을': 23, '하면서': 24, '자신이': 25, '할': 26, '수': 27, '있는': 28, '한계점을': 29, '찾고': 30, '위해': 31, '노력합시다': 32}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 문장 별 토큰화 확인\n",
    "x=token.texts_to_sequences(docs)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 1, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 31, 32]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 리뷰 감성분석"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 텍스트 리뷰 10개를 정의합니다.\n",
    "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"글쎄요 재미없어요\"]\n",
    "\n",
    "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n",
    "classes = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "token=Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "\n",
    "token"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fb3f11a9af0>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 단어별 빈도수 계산\n",
    "print(token.word_counts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('너무', 1), ('재밌네요', 1), ('최고예요', 1), ('참', 1), ('잘', 1), ('만든', 1), ('영화예요', 1), ('추천하고', 1), ('싶은', 1), ('영화입니다', 1), ('한번', 1), ('더', 1), ('보고싶네요', 1), ('글쎄요', 2), ('별로예요', 1), ('생각보다', 1), ('지루하네요', 1), ('연기가', 1), ('어색해요', 1), ('재미없어요', 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 각 단어의 인덱스\n",
    "print(token.word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'글쎄요': 1, '너무': 2, '재밌네요': 3, '최고예요': 4, '참': 5, '잘': 6, '만든': 7, '영화예요': 8, '추천하고': 9, '싶은': 10, '영화입니다': 11, '한번': 12, '더': 13, '보고싶네요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 문장 별 토큰화 확인\n",
    "x=token.texts_to_sequences(docs)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2, 3], [4], [5, 6, 7, 8], [9, 10, 11], [12, 13, 14], [1], [15], [16, 17], [18, 19], [1, 20]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 문장 최대 길이 확인\n",
    "len_list=[]\n",
    "for i in range(len(x)):\n",
    "    len_list.append(len(x[i]))\n",
    "\n",
    "print(\"문장 별 길이:\", len_list)\n",
    "print(\"문장 최대 길이:\", np.max(len_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "문장 별 길이: [2, 1, 4, 3, 3, 1, 1, 2, 2, 2]\n",
      "문장 최대 길이: 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 패딩: 서로 다른 길이의 데이터를 4로 맞추어 줍니다\n",
    "padded_x=pad_sequences(x)\n",
    "print('패딩 결과\\n', padded_x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "패딩 결과\n",
      " [[ 0  0  2  3]\n",
      " [ 0  0  0  4]\n",
      " [ 5  6  7  8]\n",
      " [ 0  9 10 11]\n",
      " [ 0 12 13 14]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  1 20]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "테스트 데이터 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "test_docs = [\"영화 재밌네요\",\"올해 영화 중 최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶지 않은 영화입니다\",\"두번 더 보고싶네요\",\"글쎄요\",\"아주 별로예요\",\"생각보다 재미없네요\",\"배우 연기가 어색해요\",\"글쎄요 재미없어요\"]\n",
    "classes_test = array([1, 1, 1, 0, 1, 0, 0, 0, 0, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "token_test=Tokenizer()\n",
    "token_test.fit_on_texts(test_docs)\n",
    "token_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fb3f8249940>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(token_test.word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'영화': 1, '글쎄요': 2, '재밌네요': 3, '올해': 4, '중': 5, '최고예요': 6, '참': 7, '잘': 8, '만든': 9, '영화예요': 10, '추천하고': 11, '싶지': 12, '않은': 13, '영화입니다': 14, '두번': 15, '더': 16, '보고싶네요': 17, '아주': 18, '별로예요': 19, '생각보다': 20, '재미없네요': 21, '배우': 22, '연기가': 23, '어색해요': 24, '재미없어요': 25}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# 문장 별 토큰화 결과 확인\n",
    "test_x = token_test.texts_to_sequences(test_docs)\n",
    "print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\",  test_x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "리뷰 텍스트, 토큰화 결과:\n",
      " [[1, 3], [4, 1, 5, 6], [7, 8, 9, 10], [11, 12, 13, 14], [15, 16, 17], [2], [18, 19], [20, 21], [22, 23, 24], [2, 25]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test_padded_x = pad_sequences(test_x, 4)  \n",
    "print(\"\\n패딩 결과:\\n\", test_padded_x) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "패딩 결과:\n",
      " [[ 0  0  1  3]\n",
      " [ 4  1  5  6]\n",
      " [ 7  8  9 10]\n",
      " [11 12 13 14]\n",
      " [ 0 15 16 17]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0 20 21]\n",
      " [ 0 22 23 24]\n",
      " [ 0  0  2 25]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 딥 러닝 모델"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# 임베딩에 입려될 단어의 수를 지정합니다. (패딩 0까지 단어로 포함하기 위해 1을 더한다)\n",
    "word_size=len(token.word_index)+1\n",
    "word_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n",
    "model = Sequential()\n",
    "\n",
    "# 임베딩 층 입력 벡터 크기: 단어 인덱스 수(21), 출력 벡터 크기: 10(수정 가능), 뉴스 패딩 결과인 4개씩 단어 입력\n",
    "model.add(Embedding(21,10,input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid')) # 이진 분류\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model.fit(padded_x, classes, epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.6972 - accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.6000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.6000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.6000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6887 - accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6866 - accuracy: 0.6000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6825 - accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6804 - accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6763 - accuracy: 0.8000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6701 - accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6681 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.9000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb3f9d1b340>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model.evaluate(padded_x, classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 124ms/step - loss: 0.6556 - accuracy: 0.9000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6556402444839478, 0.8999999761581421]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model.predict(padded_x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.51161015],\n",
       "       [0.50008875],\n",
       "       [0.5350828 ],\n",
       "       [0.5331404 ],\n",
       "       [0.5234462 ],\n",
       "       [0.49435434],\n",
       "       [0.50302386],\n",
       "       [0.47839305],\n",
       "       [0.47537407],\n",
       "       [0.45912778]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}